{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Breaks the large raw tweet data file into manageable portions\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "chunk_size = 10000\n",
    "batch_no = 1\n",
    "\n",
    "print(\"Beginning to parse tweets...\")\n",
    "\n",
    "for chunk in pd.read_csv(\"01-03_covid_data.csv\", chunksize = chunk_size, error_bad_lines=False):\n",
    "        chunk.to_csv(\"covid_data\" + str(batch_no) + \".csv\", index = False)\n",
    "        batch_no += 1\n",
    "\n",
    "print(\"Finished parsing tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date = column 6, so csv line[5] parce by one weeks into csv comlumns\n",
    "#week long windows -> datetime, 00 on a monday to midnight sundays timenexus?\n",
    "###January 27th or February 3rd -> 1 week intervals\n",
    "\n",
    "#Base on Cleo's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to create corpus...\n",
      "10000\n",
      "[\"With Crude Oil and EURUSD at near 2 month lows, Nasdaq pushing new all time highs +9000, global risks such as the Koronavirus, US/China/Iran tensions - Those of you on our online course, make sure to join tonight's 6pm live webinar  to get the full low down from the trading floor https://t.co/Bd1o2b2Qdj\", 'Novel Coronavirus 2019 Situation Summary, Wuhan, China | CDC https://t.co/BYg7ovjuPg via @CDCgov', 'RTRT @ai6yrhamCDC Traumatic Brain Injury & Concussion facts: Traumatic brain injury (TBI) is a major cause of death and disability in the United States. In 2014, there were 56,800 TBI-related deaths in the US, including 2,529 deaths among children.  https://t.co/l58QL8hFel', 'RTRT @AnneClaireCNNCDC testing several people in US for possible Wuhan virus, CDC spokesperson Kristen Nordlund says.   The CDC has the only laboratory in the United States that can test for this new virus. @cnnbrk', \"The CDC has issued a warning for the Flu this year. The b-strand is especially bad this season and worse for people w/o the vaccination. Make sure you seek medical attention and stay away from public places if you start to feel puny. Let's keep those kiddos well this season!\"]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " creates corpus for topic modelling \n",
    "'''\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "sentimentDict = {}\n",
    "documents = []\n",
    "path = \"./covid_data*\"\n",
    "\n",
    "print(\"Beginning to create corpus...\")\n",
    "#I put the smaller chunked tweet files in to a directory called revisedCovidData\n",
    "for filename in glob.glob(path):\n",
    "    #Iterate through this directory, reading each file\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as rawTweets:\n",
    "        #open as CSV iterator\n",
    "        readCSV = csv.reader(rawTweets)\n",
    "        next(readCSV)\n",
    "        #Iterate through individual tweets\n",
    "        for line in readCSV:\n",
    "            #if line[9] != \"Null\" or \"us_state\":\n",
    "                #calls text of each tweet as a TextBlob object\n",
    "            text = line[1]\n",
    "                #line[9] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "            documents.append(text)\n",
    "        break\n",
    "        \n",
    "print(len(documents))\n",
    "print(documents[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and token not in [\"rtrt\", \"https\"] and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['RTRT', '@ABCWashington', 'Gov.', 'Jay', 'Inslee', 'on', 'first', 'confirmed', 'case', 'of', 'coronavirus', 'reported', 'in', 'U.S.:', '\"The', 'risk', 'is', 'low', 'to', 'residents', 'of', 'the', 'state', 'of', 'Washington.\"', '', '\"This', 'is', 'certainly', 'not', 'a', 'moment', 'for', 'panic', 'or', 'high', 'anxiety.', 'It', 'is', 'a', 'moment', 'for', 'vigilance.\"', 'https://t.co/FsCRAdLtSR', 'https://t.co/ba26z2KHIb']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['abcwashington', 'insle', 'confirm', 'case', 'coronavirus', 'report', 'risk', 'resid', 'state', 'washington', 'certain', 'moment', 'panic', 'high', 'anxieti', 'moment', 'vigil', 'fscradltsr', 'khib']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[370]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['crude', 'eurusd', 'near', 'month', 'low', 'nasdaq', 'push', 'time', 'high', 'global', 'risk', 'koronavirus', 'china', 'iran', 'tension', 'onlin', 'cours', 'sure', 'join', 'tonight', 'live', 'webinar', 'trade', 'floor'], ['novel', 'coronavirus', 'situat', 'summari', 'wuhan', 'china', 'ovjupg', 'cdcgov'], ['yrhamcdc', 'traumat', 'brain', 'injuri', 'concuss', 'fact', 'traumat', 'brain', 'injuri', 'major', 'caus', 'death', 'disabl', 'unit', 'state', 'relat', 'death', 'includ', 'death', 'children', 'hfel'], ['test', 'peopl', 'possibl', 'wuhan', 'virus', 'spokesperson', 'kristen', 'nordlund', 'say', 'laboratori', 'unit', 'state', 'test', 'virus', 'cnnbrk'], ['issu', 'warn', 'year', 'strand', 'especi', 'season', 'wors', 'peopl', 'vaccin', 'sure', 'seek', 'medic', 'attent', 'stay', 'away', 'public', 'place', 'start', 'feel', 'puni', 'kiddo', 'season'], ['kristin', 'accord', 'death', 'measl', 'decad', 'studi', 'report', 'whatarewedo', 'voteno', 'voteno', 'votenoa', 'voteno', 'mybodymychoic', 'vaccineinjuri', 'riseup', 'wakeup', 'thisisjersey'], ['anonnonsens', 'rout', 'flight', 'airport', 'screen'], ['sourc', 'sirahx'], ['stakehold', 'engag', 'communic', 'thursday', 'februari', 'eastern', 'time', 'great', 'britain', 'ireland', 'lyzew', 'mecf', 'myalgic', 'pwme', 'xodrkc'], ['share', 'help', 'tip', 'carbon', 'monoxid', 'poison', 'prevent', 'home', 'rxychj']]\n",
      "[['crude', 'eurusd', 'near', 'month', 'low', 'nasdaq', 'push', 'time', 'high', 'global', 'risk', 'koronavirus', 'china', 'iran', 'tension', 'onlin', 'cours', 'sure', 'join', 'tonight', 'live', 'webinar', 'trade', 'floor'], ['novel', 'coronavirus', 'situat', 'summari', 'wuhan', 'china', 'ovjupg', 'cdcgov'], ['yrhamcdc', 'traumat', 'brain', 'injuri', 'concuss', 'fact', 'traumat', 'brain', 'injuri', 'major', 'caus', 'death', 'disabl', 'unit', 'state', 'relat', 'death', 'includ', 'death', 'children', 'hfel'], ['test', 'peopl', 'possibl', 'wuhan', 'virus', 'spokesperson', 'kristen', 'nordlund', 'say', 'laboratori', 'unit', 'state', 'test', 'virus', 'cnnbrk'], ['issu', 'warn', 'year', 'strand', 'especi', 'season', 'wors', 'peopl', 'vaccin', 'sure', 'seek', 'medic', 'attent', 'stay', 'away', 'public', 'place', 'start', 'feel', 'puni', 'kiddo', 'season'], ['kristin', 'accord', 'death', 'measl', 'decad', 'studi', 'report', 'whatarewedo', 'voteno', 'voteno', 'votenoa', 'voteno', 'mybodymychoic', 'vaccineinjuri', 'riseup', 'wakeup', 'thisisjersey'], ['anonnonsens', 'rout', 'flight', 'airport', 'screen'], ['sourc', 'sirahx'], ['stakehold', 'engag', 'communic', 'thursday', 'februari', 'eastern', 'time', 'great', 'britain', 'ireland', 'lyzew', 'mecf', 'myalgic', 'pwme', 'xodrkc'], ['share', 'help', 'tip', 'carbon', 'monoxid', 'poison', 'prevent', 'home', 'rxychj']]\n",
      "[['crude', 'eurusd', 'near', 'month', 'low', 'nasdaq', 'push', 'time', 'high', 'global', 'risk', 'koronavirus', 'china', 'iran', 'tension', 'onlin', 'cours', 'sure', 'join', 'tonight', 'live', 'webinar', 'trade', 'floor'], ['novel', 'coronavirus', 'situat', 'summari', 'wuhan', 'china', 'ovjupg', 'cdcgov'], ['yrhamcdc', 'traumat', 'brain', 'injuri', 'concuss', 'fact', 'traumat', 'brain', 'injuri', 'major', 'caus', 'death', 'disabl', 'unit', 'state', 'relat', 'death', 'includ', 'death', 'children', 'hfel'], ['test', 'peopl', 'possibl', 'wuhan', 'virus', 'spokesperson', 'kristen', 'nordlund', 'say', 'laboratori', 'unit', 'state', 'test', 'virus', 'cnnbrk'], ['issu', 'warn', 'year', 'strand', 'especi', 'season', 'wors', 'peopl', 'vaccin', 'sure', 'seek', 'medic', 'attent', 'stay', 'away', 'public', 'place', 'start', 'feel', 'puni', 'kiddo', 'season'], ['kristin', 'accord', 'death', 'measl', 'decad', 'studi', 'report', 'whatarewedo', 'voteno', 'voteno', 'votenoa', 'voteno', 'mybodymychoic', 'vaccineinjuri', 'riseup', 'wakeup', 'thisisjersey'], ['anonnonsens', 'rout', 'flight', 'airport', 'screen'], ['sourc', 'sirahx'], ['stakehold', 'engag', 'communic', 'thursday', 'februari', 'eastern', 'time', 'great', 'britain', 'ireland', 'lyzew', 'mecf', 'myalgic', 'pwme', 'xodrkc'], ['share', 'help', 'tip', 'carbon', 'monoxid', 'poison', 'prevent', 'home', 'rxychj']]\n",
      "[['crude', 'eurusd', 'near', 'month', 'low', 'nasdaq', 'push', 'time', 'high', 'global', 'risk', 'koronavirus', 'china', 'iran', 'tension', 'onlin', 'cours', 'sure', 'join', 'tonight', 'live', 'webinar', 'trade', 'floor'], ['novel', 'coronavirus', 'situat', 'summari', 'wuhan', 'china', 'ovjupg', 'cdcgov'], ['yrhamcdc', 'traumat', 'brain', 'injuri', 'concuss', 'fact', 'traumat', 'brain', 'injuri', 'major', 'caus', 'death', 'disabl', 'unit', 'state', 'relat', 'death', 'includ', 'death', 'children', 'hfel'], ['test', 'peopl', 'possibl', 'wuhan', 'virus', 'spokesperson', 'kristen', 'nordlund', 'say', 'laboratori', 'unit', 'state', 'test', 'virus', 'cnnbrk'], ['issu', 'warn', 'year', 'strand', 'especi', 'season', 'wors', 'peopl', 'vaccin', 'sure', 'seek', 'medic', 'attent', 'stay', 'away', 'public', 'place', 'start', 'feel', 'puni', 'kiddo', 'season'], ['kristin', 'accord', 'death', 'measl', 'decad', 'studi', 'report', 'whatarewedo', 'voteno', 'voteno', 'votenoa', 'voteno', 'mybodymychoic', 'vaccineinjuri', 'riseup', 'wakeup', 'thisisjersey'], ['anonnonsens', 'rout', 'flight', 'airport', 'screen'], ['sourc', 'sirahx'], ['stakehold', 'engag', 'communic', 'thursday', 'februari', 'eastern', 'time', 'great', 'britain', 'ireland', 'lyzew', 'mecf', 'myalgic', 'pwme', 'xodrkc'], ['share', 'help', 'tip', 'carbon', 'monoxid', 'poison', 'prevent', 'home', 'rxychj']]\n",
      "[['crude', 'eurusd', 'near', 'month', 'low', 'nasdaq', 'push', 'time', 'high', 'global', 'risk', 'koronavirus', 'china', 'iran', 'tension', 'onlin', 'cours', 'sure', 'join', 'tonight', 'live', 'webinar', 'trade', 'floor'], ['novel', 'coronavirus', 'situat', 'summari', 'wuhan', 'china', 'ovjupg', 'cdcgov'], ['yrhamcdc', 'traumat', 'brain', 'injuri', 'concuss', 'fact', 'traumat', 'brain', 'injuri', 'major', 'caus', 'death', 'disabl', 'unit', 'state', 'relat', 'death', 'includ', 'death', 'children', 'hfel'], ['test', 'peopl', 'possibl', 'wuhan', 'virus', 'spokesperson', 'kristen', 'nordlund', 'say', 'laboratori', 'unit', 'state', 'test', 'virus', 'cnnbrk'], ['issu', 'warn', 'year', 'strand', 'especi', 'season', 'wors', 'peopl', 'vaccin', 'sure', 'seek', 'medic', 'attent', 'stay', 'away', 'public', 'place', 'start', 'feel', 'puni', 'kiddo', 'season'], ['kristin', 'accord', 'death', 'measl', 'decad', 'studi', 'report', 'whatarewedo', 'voteno', 'voteno', 'votenoa', 'voteno', 'mybodymychoic', 'vaccineinjuri', 'riseup', 'wakeup', 'thisisjersey'], ['anonnonsens', 'rout', 'flight', 'airport', 'screen'], ['sourc', 'sirahx'], ['stakehold', 'engag', 'communic', 'thursday', 'februari', 'eastern', 'time', 'great', 'britain', 'ireland', 'lyzew', 'mecf', 'myalgic', 'pwme', 'xodrkc'], ['share', 'help', 'tip', 'carbon', 'monoxid', 'poison', 'prevent', 'home', 'rxychj']]\n",
      "[['crude', 'eurusd', 'near', 'month', 'low', 'nasdaq', 'push', 'time', 'high', 'global', 'risk', 'koronavirus', 'china', 'iran', 'tension', 'onlin', 'cours', 'sure', 'join', 'tonight', 'live', 'webinar', 'trade', 'floor'], ['novel', 'coronavirus', 'situat', 'summari', 'wuhan', 'china', 'ovjupg', 'cdcgov'], ['yrhamcdc', 'traumat', 'brain', 'injuri', 'concuss', 'fact', 'traumat', 'brain', 'injuri', 'major', 'caus', 'death', 'disabl', 'unit', 'state', 'relat', 'death', 'includ', 'death', 'children', 'hfel'], ['test', 'peopl', 'possibl', 'wuhan', 'virus', 'spokesperson', 'kristen', 'nordlund', 'say', 'laboratori', 'unit', 'state', 'test', 'virus', 'cnnbrk'], ['issu', 'warn', 'year', 'strand', 'especi', 'season', 'wors', 'peopl', 'vaccin', 'sure', 'seek', 'medic', 'attent', 'stay', 'away', 'public', 'place', 'start', 'feel', 'puni', 'kiddo', 'season'], ['kristin', 'accord', 'death', 'measl', 'decad', 'studi', 'report', 'whatarewedo', 'voteno', 'voteno', 'votenoa', 'voteno', 'mybodymychoic', 'vaccineinjuri', 'riseup', 'wakeup', 'thisisjersey'], ['anonnonsens', 'rout', 'flight', 'airport', 'screen'], ['sourc', 'sirahx'], ['stakehold', 'engag', 'communic', 'thursday', 'februari', 'eastern', 'time', 'great', 'britain', 'ireland', 'lyzew', 'mecf', 'myalgic', 'pwme', 'xodrkc'], ['share', 'help', 'tip', 'carbon', 'monoxid', 'poison', 'prevent', 'home', 'rxychj']]\n",
      "[['crude', 'eurusd', 'near', 'month', 'low', 'nasdaq', 'push', 'time', 'high', 'global', 'risk', 'koronavirus', 'china', 'iran', 'tension', 'onlin', 'cours', 'sure', 'join', 'tonight', 'live', 'webinar', 'trade', 'floor'], ['novel', 'coronavirus', 'situat', 'summari', 'wuhan', 'china', 'ovjupg', 'cdcgov'], ['yrhamcdc', 'traumat', 'brain', 'injuri', 'concuss', 'fact', 'traumat', 'brain', 'injuri', 'major', 'caus', 'death', 'disabl', 'unit', 'state', 'relat', 'death', 'includ', 'death', 'children', 'hfel'], ['test', 'peopl', 'possibl', 'wuhan', 'virus', 'spokesperson', 'kristen', 'nordlund', 'say', 'laboratori', 'unit', 'state', 'test', 'virus', 'cnnbrk'], ['issu', 'warn', 'year', 'strand', 'especi', 'season', 'wors', 'peopl', 'vaccin', 'sure', 'seek', 'medic', 'attent', 'stay', 'away', 'public', 'place', 'start', 'feel', 'puni', 'kiddo', 'season'], ['kristin', 'accord', 'death', 'measl', 'decad', 'studi', 'report', 'whatarewedo', 'voteno', 'voteno', 'votenoa', 'voteno', 'mybodymychoic', 'vaccineinjuri', 'riseup', 'wakeup', 'thisisjersey'], ['anonnonsens', 'rout', 'flight', 'airport', 'screen'], ['sourc', 'sirahx'], ['stakehold', 'engag', 'communic', 'thursday', 'februari', 'eastern', 'time', 'great', 'britain', 'ireland', 'lyzew', 'mecf', 'myalgic', 'pwme', 'xodrkc'], ['share', 'help', 'tip', 'carbon', 'monoxid', 'poison', 'prevent', 'home', 'rxychj']]\n",
      "[['crude', 'eurusd', 'near', 'month', 'low', 'nasdaq', 'push', 'time', 'high', 'global', 'risk', 'koronavirus', 'china', 'iran', 'tension', 'onlin', 'cours', 'sure', 'join', 'tonight', 'live', 'webinar', 'trade', 'floor'], ['novel', 'coronavirus', 'situat', 'summari', 'wuhan', 'china', 'ovjupg', 'cdcgov'], ['yrhamcdc', 'traumat', 'brain', 'injuri', 'concuss', 'fact', 'traumat', 'brain', 'injuri', 'major', 'caus', 'death', 'disabl', 'unit', 'state', 'relat', 'death', 'includ', 'death', 'children', 'hfel'], ['test', 'peopl', 'possibl', 'wuhan', 'virus', 'spokesperson', 'kristen', 'nordlund', 'say', 'laboratori', 'unit', 'state', 'test', 'virus', 'cnnbrk'], ['issu', 'warn', 'year', 'strand', 'especi', 'season', 'wors', 'peopl', 'vaccin', 'sure', 'seek', 'medic', 'attent', 'stay', 'away', 'public', 'place', 'start', 'feel', 'puni', 'kiddo', 'season'], ['kristin', 'accord', 'death', 'measl', 'decad', 'studi', 'report', 'whatarewedo', 'voteno', 'voteno', 'votenoa', 'voteno', 'mybodymychoic', 'vaccineinjuri', 'riseup', 'wakeup', 'thisisjersey'], ['anonnonsens', 'rout', 'flight', 'airport', 'screen'], ['sourc', 'sirahx'], ['stakehold', 'engag', 'communic', 'thursday', 'februari', 'eastern', 'time', 'great', 'britain', 'ireland', 'lyzew', 'mecf', 'myalgic', 'pwme', 'xodrkc'], ['share', 'help', 'tip', 'carbon', 'monoxid', 'poison', 'prevent', 'home', 'rxychj']]\n",
      "[['crude', 'eurusd', 'near', 'month', 'low', 'nasdaq', 'push', 'time', 'high', 'global', 'risk', 'koronavirus', 'china', 'iran', 'tension', 'onlin', 'cours', 'sure', 'join', 'tonight', 'live', 'webinar', 'trade', 'floor'], ['novel', 'coronavirus', 'situat', 'summari', 'wuhan', 'china', 'ovjupg', 'cdcgov'], ['yrhamcdc', 'traumat', 'brain', 'injuri', 'concuss', 'fact', 'traumat', 'brain', 'injuri', 'major', 'caus', 'death', 'disabl', 'unit', 'state', 'relat', 'death', 'includ', 'death', 'children', 'hfel'], ['test', 'peopl', 'possibl', 'wuhan', 'virus', 'spokesperson', 'kristen', 'nordlund', 'say', 'laboratori', 'unit', 'state', 'test', 'virus', 'cnnbrk'], ['issu', 'warn', 'year', 'strand', 'especi', 'season', 'wors', 'peopl', 'vaccin', 'sure', 'seek', 'medic', 'attent', 'stay', 'away', 'public', 'place', 'start', 'feel', 'puni', 'kiddo', 'season'], ['kristin', 'accord', 'death', 'measl', 'decad', 'studi', 'report', 'whatarewedo', 'voteno', 'voteno', 'votenoa', 'voteno', 'mybodymychoic', 'vaccineinjuri', 'riseup', 'wakeup', 'thisisjersey'], ['anonnonsens', 'rout', 'flight', 'airport', 'screen'], ['sourc', 'sirahx'], ['stakehold', 'engag', 'communic', 'thursday', 'februari', 'eastern', 'time', 'great', 'britain', 'ireland', 'lyzew', 'mecf', 'myalgic', 'pwme', 'xodrkc'], ['share', 'help', 'tip', 'carbon', 'monoxid', 'poison', 'prevent', 'home', 'rxychj']]\n",
      "[['crude', 'eurusd', 'near', 'month', 'low', 'nasdaq', 'push', 'time', 'high', 'global', 'risk', 'koronavirus', 'china', 'iran', 'tension', 'onlin', 'cours', 'sure', 'join', 'tonight', 'live', 'webinar', 'trade', 'floor'], ['novel', 'coronavirus', 'situat', 'summari', 'wuhan', 'china', 'ovjupg', 'cdcgov'], ['yrhamcdc', 'traumat', 'brain', 'injuri', 'concuss', 'fact', 'traumat', 'brain', 'injuri', 'major', 'caus', 'death', 'disabl', 'unit', 'state', 'relat', 'death', 'includ', 'death', 'children', 'hfel'], ['test', 'peopl', 'possibl', 'wuhan', 'virus', 'spokesperson', 'kristen', 'nordlund', 'say', 'laboratori', 'unit', 'state', 'test', 'virus', 'cnnbrk'], ['issu', 'warn', 'year', 'strand', 'especi', 'season', 'wors', 'peopl', 'vaccin', 'sure', 'seek', 'medic', 'attent', 'stay', 'away', 'public', 'place', 'start', 'feel', 'puni', 'kiddo', 'season'], ['kristin', 'accord', 'death', 'measl', 'decad', 'studi', 'report', 'whatarewedo', 'voteno', 'voteno', 'votenoa', 'voteno', 'mybodymychoic', 'vaccineinjuri', 'riseup', 'wakeup', 'thisisjersey'], ['anonnonsens', 'rout', 'flight', 'airport', 'screen'], ['sourc', 'sirahx'], ['stakehold', 'engag', 'communic', 'thursday', 'februari', 'eastern', 'time', 'great', 'britain', 'ireland', 'lyzew', 'mecf', 'myalgic', 'pwme', 'xodrkc'], ['share', 'help', 'tip', 'carbon', 'monoxid', 'poison', 'prevent', 'home', 'rxychj']]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_docs = []\n",
    "for tweet in documents[:10]:\n",
    "    thing = preprocess(tweet)\n",
    "    preprocessed_docs.append(thing)\n",
    "type(preprocessed_docs)\n",
    "i = 0\n",
    "for element in preprocessed_docs:\n",
    "    i+=1\n",
    "    print(preprocessed_docs)\n",
    "    if i >= 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['crude', 'eurusd', 'near', 'month', 'low', 'nasdaq', 'push', 'time', 'high', 'global', 'risk', 'koronavirus', 'china', 'iran', 'tension', 'onlin', 'cours', 'sure', 'join', 'tonight', 'live', 'webinar', 'trade', 'floor'], ['novel', 'coronavirus', 'situat', 'summari', 'wuhan', 'china', 'ovjupg', 'cdcgov'], ['yrhamcdc', 'traumat', 'brain', 'injuri', 'concuss', 'fact', 'traumat', 'brain', 'injuri', 'major', 'caus', 'death', 'disabl', 'unit', 'state', 'relat', 'death', 'includ', 'death', 'children', 'hfel'], ['test', 'peopl', 'possibl', 'wuhan', 'virus', 'spokesperson', 'kristen', 'nordlund', 'say', 'laboratori', 'unit', 'state', 'test', 'virus', 'cnnbrk'], ['issu', 'warn', 'year', 'strand', 'especi', 'season', 'wors', 'peopl', 'vaccin', 'sure', 'seek', 'medic', 'attent', 'stay', 'away', 'public', 'place', 'start', 'feel', 'puni', 'kiddo', 'season'], ['kristin', 'accord', 'death', 'measl', 'decad', 'studi', 'report', 'whatarewedo', 'voteno', 'voteno', 'votenoa', 'voteno', 'mybodymychoic', 'vaccineinjuri', 'riseup', 'wakeup', 'thisisjersey'], ['anonnonsens', 'rout', 'flight', 'airport', 'screen'], ['sourc', 'sirahx'], ['stakehold', 'engag', 'communic', 'thursday', 'februari', 'eastern', 'time', 'great', 'britain', 'ireland', 'lyzew', 'mecf', 'myalgic', 'pwme', 'xodrkc'], ['share', 'help', 'tip', 'carbon', 'monoxid', 'poison', 'prevent', 'home', 'rxychj']]\n",
      "0 china\n",
      "1 cours\n",
      "2 crude\n",
      "3 eurusd\n",
      "4 floor\n",
      "5 global\n",
      "6 high\n",
      "7 iran\n",
      "8 join\n",
      "9 koronavirus\n",
      "10 live\n"
     ]
    }
   ],
   "source": [
    "print((preprocessed_docs))\n",
    "dictionary = gensim.corpora.Dictionary(preprocessed_docs)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters out any token in 15 or fewer, more than half, and only the 100,000 most common\n",
    "#dictionary.filter_extremes(no_below = 15, no_above = 0.5, keep_n = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'peopl', 'possibl', 'wuhan', 'virus', 'spokesperson', 'kristen', 'nordlund', 'say', 'laboratori', 'unit', 'state', 'test', 'virus', 'cnnbrk']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_docs[3])\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 35 (\"death\") appears 1 time.\n",
      "Word 76 (\"accord\") appears 1 time.\n",
      "Word 77 (\"decad\") appears 1 time.\n",
      "Word 78 (\"kristin\") appears 1 time.\n",
      "Word 79 (\"measl\") appears 1 time.\n",
      "Word 80 (\"mybodymychoic\") appears 1 time.\n",
      "Word 81 (\"report\") appears 1 time.\n",
      "Word 82 (\"riseup\") appears 1 time.\n",
      "Word 83 (\"studi\") appears 1 time.\n",
      "Word 84 (\"thisisjersey\") appears 1 time.\n",
      "Word 85 (\"vaccineinjuri\") appears 1 time.\n",
      "Word 86 (\"voteno\") appears 3 time.\n",
      "Word 87 (\"votenoa\") appears 1 time.\n",
      "Word 88 (\"wakeup\") appears 1 time.\n",
      "Word 89 (\"whatarewedo\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_5 = bow_corpus[5]\n",
    "for i in range(len(bow_doc_5)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_5[i][0], \n",
    "                                               dictionary[bow_doc_5[i][0]], \n",
    "bow_doc_5[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.14746833660826209),\n",
      " (1, 0.210979492243517),\n",
      " (2, 0.210979492243517),\n",
      " (3, 0.210979492243517),\n",
      " (4, 0.210979492243517),\n",
      " (5, 0.210979492243517),\n",
      " (6, 0.210979492243517),\n",
      " (7, 0.210979492243517),\n",
      " (8, 0.210979492243517),\n",
      " (9, 0.210979492243517),\n",
      " (10, 0.210979492243517),\n",
      " (11, 0.210979492243517),\n",
      " (12, 0.210979492243517),\n",
      " (13, 0.210979492243517),\n",
      " (14, 0.210979492243517),\n",
      " (15, 0.210979492243517),\n",
      " (16, 0.210979492243517),\n",
      " (17, 0.210979492243517),\n",
      " (18, 0.14746833660826209),\n",
      " (19, 0.210979492243517),\n",
      " (20, 0.14746833660826209),\n",
      " (21, 0.210979492243517),\n",
      " (22, 0.210979492243517),\n",
      " (23, 0.210979492243517)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-80-c7ba9ae6e92c>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-80-c7ba9ae6e92c>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    for idx, topic in lda_model_tfidf.print_topics(-1):\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topiclda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs[370]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[370]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build LDA model\n",
    "'''\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics = 5,\n",
    "                                           random_state = 100 \n",
    "                                           update_every=1,\n",
    "                                           chunksize = 100,\n",
    "                                           passes = 10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure how good the model is. Lower is better\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "#compute coherence score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts = data_lemmatized, dictionary = id2word, coherence='c_V')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "help(dt.timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to assign sentiments...\n",
      "Finished assigning sentiments.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Creates a dictionary mapping the value of \"state\" in each tweet with the average sentiment\n",
    "\n",
    "senitiment.polarity: denotes the sentiment of a text, float from -1 (completely negative) to 1 (completely positive) \n",
    "\n",
    "'''\n",
    "import datetime as dt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import textblob\n",
    "import glob\n",
    "\n",
    "sentimentDict = {}\n",
    "tweetList = []\n",
    "path = \"./covid_data*\"\n",
    "\n",
    "print(\"Beginning to assign sentiments...\")\n",
    "#I put the smaller chunked tweet files in to a directory called revisedCovidData\n",
    "for filename in glob.glob(path):\n",
    "    #Iterate through this directory, reading each file\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as rawTweets:\n",
    "        #open as CSV iterator\n",
    "        readCSV = csv.reader(rawTweets)\n",
    "        #Iterate through individual tweets\n",
    "        for line in readCSV:\n",
    "            if line[9] != \"Null\" or \"us_state\":\n",
    "                #calls text of each tweet as a TextBlob object\n",
    "                text = textblob.TextBlob(line[1])\n",
    "                #line[9] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                if line[9] in sentimentDict:\n",
    "                    #sentimentDict[line[9]] = (sentimentDict[line[9]][1] +  (text.sentiment.polarity))/2\n",
    "                    sentimentDict[line[9]].append(text.sentiment.polarity) #<= if wanting a list instead of an average\n",
    "                else:\n",
    "                    #it state not already accounted for, saves it as a key with the relevant sentiment\n",
    "                    sentimentDict.update({line[9]: [(text.sentiment.polarity)]})\n",
    "                sentimentDict[line[9]].append(c1.classify(line[1]))\n",
    "                \n",
    "    break\n",
    "\n",
    "#denote finished. These are large files, it takes a while\n",
    "print(\"Finished assigning sentiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates a dictionary mapping the value of \"state\" in each tweet with the average sentiment\n",
    "\n",
    "senitiment.polarity: denotes the sentiment of a text, float from -1 (completely negative) to 1 (completely positive) \n",
    "\n",
    "STILL NEEDS: sort by topic and sort into sliding window by 1 week intervals\n",
    "\n",
    "'''\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "outfile = \"TweetTextOnly.txt\"\n",
    "tweetList = []\n",
    "path = \"./covid_data*\"\n",
    "\n",
    "print(\"Beginning to assign sentiments...\")\n",
    "#I put the smaller chunked tweet files in to a directory called revisedCovidData\n",
    "for filename in glob.glob(path):\n",
    "    #Iterate through this directory, reading each file\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as rawTweets:\n",
    "        #open as CSV iterator\n",
    "        readCSV = csv.reader(rawTweets)\n",
    "        #Iterate through individual tweets\n",
    "        for line in readCSV:\n",
    "            if line[9] != \"Null\" or \"us_state\":\n",
    "                #calls text of each tweet as a TextBlob object\n",
    "                text = textblob.TextBlob(line[1])\n",
    "                #line[9] = state; if this state is already in the dictionary, the sentiment gets averaged\n",
    "                if line[9] in sentimentDict:\n",
    "                    sentimentDict[line[9]] = (sentimentDict[line[9]][1] +  (text.sentiment.polarity))/2\n",
    "                    #sentimentDict[line[9]].append(text.sentiment.polarity) #<= if wanting a list instead of an average\n",
    "                else:\n",
    "                    #it state not already accounted for, saves it as a key with the relevant sentiment\n",
    "                    sentimentDict.update({line[9]: (text.sentiment.polarity)})\n",
    "                \n",
    "                \n",
    "    break\n",
    "\n",
    "#denote finished. These are large files, it takes a while\n",
    "print(\"Finished assigning sentiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 0 Date Range Sentiment State Topic\n",
      "0                                0        NaN       NaN   NaN   NaN\n",
      "1   Wed Jan 22 17:17:16 +0000 2020        NaN       NaN   NaN   NaN\n",
      "2                             Null        NaN       NaN   NaN   NaN\n",
      "3                             NULL        NaN       NaN   NaN   NaN\n",
      "4                        0.0367172        NaN       NaN   NaN   NaN\n",
      "..                             ...        ...       ...   ...   ...\n",
      "0                             9999        NaN       NaN   NaN   NaN\n",
      "1   Thu Jan 23 15:41:43 +0000 2020        NaN       NaN   NaN   NaN\n",
      "2                             Null        NaN       NaN   NaN   NaN\n",
      "3                             NULL        NaN       NaN   NaN   NaN\n",
      "4                              0.1        NaN       NaN   NaN   NaN\n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import textblob\n",
    "import glob\n",
    "\n",
    "sentimentDict = {}\n",
    "tweetList = []\n",
    "path = \"./covid_data*\"\n",
    "\n",
    "column_names = [\"Date Range\", \"State\", \"Topic\", \"Sentiment\"]\n",
    "analysis = pd.DataFrame(columns = column_names)\n",
    "\n",
    "for filename in glob.glob(path):\n",
    "    df = pd.read_csv(filename)\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"us_state\"] != \"Null\" or \"us_state\":\n",
    "            text = textblob.TextBlob(row[\"text\"])\n",
    "            sentiment = text.sentiment.polarity\n",
    "            current_analysis = ([index, row[\"created_at\"], row[\"us_state\"], \"NULL\", sentiment])\n",
    "            #analysis = analysis.append(current_analysis), fill in by cell\n",
    "            #need to format correctly\n",
    "    break\n",
    "\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date range(1 week)   state   topic(from modelling),   sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing sentiments to file...\n",
      "finsished writing to file\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "(Optional) writes dictionary to CSV file with rows of state, sentiment\n",
    "\"\"\"\n",
    "\n",
    "print(\"writing sentiments to file...\")\n",
    "with open(\"twitter_sentiments_byTopic.csv\", \"w\") as outFile:\n",
    "    writer = csv.writer(outFile)\n",
    "    writer.writerow([\"State\", \"Avg. Sentiment\"])\n",
    "    for key, value in sentimentDict.items():\n",
    "        writer.writerow([key, value])\n",
    "print(\"finsished writing to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
